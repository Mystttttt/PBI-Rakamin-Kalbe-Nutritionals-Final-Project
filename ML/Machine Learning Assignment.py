# -*- coding: utf-8 -*-
"""Untitled17.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1clPIYfHPAR_tEgLEkUI__nXK2NpaejpF

### **Importing Library**
"""

!pip install pmdarima

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import pmdarima as pm
from tabulate import tabulate
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt
from statsmodels.tsa.seasonal import seasonal_decompose
from pandas.plotting import autocorrelation_plot
from sklearn import preprocessing
from sklearn.cluster import KMeans
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error

import warnings
warnings.filterwarnings("ignore")

"""## **Time series**"""

# reading dataset
customers_df = pd.read_csv('Customer.csv', sep =';')
product_df = pd.read_csv('Product.csv', sep =';')
store_df = pd.read_csv('Store.csv', sep =';')
transaction_df = pd.read_csv('Transaction.csv', sep =';')

store_df.head()

customers_df.head()

product_df.head()

transaction_df.head()

#Check Shape
print("customer shape ",customers_df.shape)
print("product shape ",product_df.shape)
print("store shape ",store_df.shape)
print("transaction shape ",transaction_df.shape)

#Customer Data
customers_df['Income'] = customers_df['Income'].replace('[,]','.', regex=True).astype('float')
#Store Data
store_df['Latitude'] = store_df['Latitude'].replace('[,]','.', regex=True).astype('float')
store_df['Longitude'] = store_df['Longitude'].replace('[,]','.', regex=True).astype('float')

#Transaction Data
transaction_df['Date'] = pd.to_datetime(transaction_df['Date'])

#Merge Data
df_merge = pd.merge(transaction_df, customers_df, on=['CustomerID'])
df_merge = pd.merge(df_merge, product.drop(columns=['Price']), on=['ProductID'])
df_merge = pd.merge(df_merge, store_df, on=['StoreID'])

df_merge.head()

df_reggression = df_merge.groupby(['Date']).agg({
    'Qty':'sum'}
).reset_index()

df_reggression

#period = 12
decomposed = seasonal_decompose (df_reggression.set_index("Date"))
plt.figure(figsize=(8, 20))
plt.subplot(311)
decomposed.trend.plot(ax=plt.gca())
plt.title('Trend')
plt.subplot(312)
decomposed.seasonal.plot(ax=plt.gca())
plt.title('Seasonality')
plt.subplot(313)
decomposed.resid.plot(ax=plt.gca())
plt.title( 'Residuals')
plt.tight_layout

cut_off = round(df_reggression.shape[0] * 0.9)
train = df_reggression[:cut_off]
test = df_reggression[cut_off:].reset_index(drop=True)
train.shape, test.shape

plt.figure(figsize=(20,6))
sns.lineplot(data=train, x=train['Date'], y=train['Qty'])
sns.lineplot(data=test, x=test['Date'], y=test['Qty'])

autocorrelation_plot(df_reggression['Qty'])

def rase(y_actual, y_pred): #Func to calculate MSE
  print(f'RMSE Value {mean_squared_error(y_actual, y_pred)**0.5}')

def eval (y_actual, y_pred): #Eval Function ML modelling

  rase(y_actual, y_pred)
  print(f'MAE Value {mean_absolute_error, y_actual, y_pred}')

#ARIMA

train = train.set_index('Date')
test = test.set_index('Date')

y = train['Qty']

ARIMAmodel = ARIMA(y, order = (40, 2, 1))
ARIMAmodel = ARIMAmodel.fit()

y_pred = ARIMAmodel.get_forecast(len(test))

y_pred_df = y_pred.conf_int()
y_pred_df['Predictions'] = ARIMAmodel.predict(start=y_pred_df.index[0], end=y_pred_df.index[-1])
y_pred_df.index = test.index
y_pred_out = y_pred_df['Predictions']
eval(test['Qty'], y_pred_out)

plt.figure(figsize=(20, 5))
plt.plot(train['Qty'])
plt.plot(test['Qty'], color='red')
plt.plot(y_pred_out, color='black', label = "Arima Predict")
plt.legend()

"""## **Cluster**"""

df_merge.head()

df_cluster = df_merge.groupby(['CustomerID']).agg({'TransactionID':'count', 'Qty': 'sum', 'TotalAmount':'sum'}).reset_index()

df_cluster.head()

df_cluster

cluster_data = df_cluster.drop(columns=['CustomerID'])
cluster_data_normalize = preprocessing.normalize(cluster_data)

cluster_data_normalize

#KMeans Approach

K = range(2,8)
fits = []
score = []
for k in K:
  model = KMeans(n_clusters = k, random_state = 0, n_init='auto').fit(cluster_data_normalize)
  fits.append(model)
  score.append(silhouette_score(cluster_data_normalize, model.labels_, metric='euclidean'))

sns.lineplot(x = K, y = score);

df_cluster['cluster_label'] = fits[2].labels_

df_cluster.groupby(['cluster_label']).agg({
    'CustomerID' : 'count',
    'TransactionID' : 'mean',
    'Qty' : 'mean', 'TotalAmount' : 'mean'
})